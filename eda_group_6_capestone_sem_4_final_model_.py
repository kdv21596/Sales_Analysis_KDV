# -*- coding: utf-8 -*-
"""EDA_Group_6_Capestone_Sem_4_Final_Model_.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11tU1UlSxWF_7XtgQ2qGfNOqkBzo_WI0C

### Drive mount for dataset access
"""

from google.colab import drive
drive.mount('/content/drive')

"""### Importing libraries"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import seaborn as sns

"""### Import Dataset"""

df = pd.read_excel('/content/drive/MyDrive/Kenya_Data.xlsx')

"""### **Dataset Overview:**

Let's take a look at the first few rows of your dataset:
"""

df.T

df.columns

drop_coumns = ['Coupon Code', 'Discount Amount', 'Item #', 'Discount Amount Tax','Last Name (Billing)', 'Company (Billing)', 'Address 1&2 (Billing)', 'Postcode (Billing)','Country Code (Billing)', 'Customer Note', 'Email (Billing)', 'Phone (Billing)', 'Last Name (Shipping)', 'Address 1&2 (Shipping)', 'State Code (Shipping)','Postcode (Shipping)', 'Country Code (Shipping)', 'Cart Discount Amount', 'Order Total Tax Amount']
df = df.drop(columns=drop_coumns)

df

"""### Checking NaN Values in Data"""

nan_columns = df.isnull().sum()
print("Columns with NaN values:")
print(nan_columns[nan_columns > 0])

"""## Handled NaN Values as 0 (Zero)"""

columns_to_replace_with_zero = ['Shipping Method Title', 'SKU']

df[columns_to_replace_with_zero] = df[columns_to_replace_with_zero].fillna(0)

df.T

companies = ['Apple', 'Cursor', 'HP', 'Nokia', 'Oppo', 'Samsung', 'Epson', 'Acer', 'Von', 'Hikvision', 'Toshiba', 'Lexar', 'Display', 'Crucial', 'Vision Plus', 'Sony', 'Mi', 'Transcend', 'Infinix', 'Realme', 'Oneplus', 'Google', 'LG', 'EcoTank', 'Test', 'iTel', 'Vivo', 'Xiaomi', ]
df['Company'] = df['Item Name'].apply(lambda x: next((company for company in companies if company in x), 'Other_Company'))

df.head().T

column_counts = len(df.columns)

row_counts = len(df)

print("No. of Columns : ", column_counts)
print("No. of Rows : ", row_counts)

"""### **Descriptive Statistics:**

Use summary statistics like mean, median, mode, range, and standard deviation to describe the central tendency and spread of numeric features.
"""

df.describe()

"""### **Handling Missing Data:**

Identify and handle missing data. You can check for missing values in your dataset using the following:
"""

# Check for missing values
df.isnull().sum()

"""### *Correlation Analysis:*

Interpretation of the correlation matrix:



*   A value close to 1 indicates a strong positive correlation.
*   A value close to -1 indicates a strong negative correlation.
*   A value close to 0 indicates a weak or no correlation.

"""

# Check for missing values
missing_values = df.isnull().sum()
if missing_values.sum() > 0:
    # Handle missing values, e.g., fill with median or drop rows/columns
    df = df.fillna(df.median())  # Fill missing values with median

# Ensure numeric data types
numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns
numeric_df = df[numeric_columns]

# Calculate correlation matrix
correlation_matrix = numeric_df.corr()

# Plotting the correlation matrix as a heatmap
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.show()

"""### Box Plot for Outlier Detection:"""

# Box Plot with Outliers

plt.figure(figsize=(8, 6))
ax = sns.boxplot(x=df['Order Total Amount'], color='salmon')

# Customize x-axis ticks
tick_values = np.arange(0, df['Order Total Amount'].max() + 1000000, 1000000)
ax.set_xticks(tick_values)
ax.set_xticklabels([f'{val:,}' for val in tick_values])  # Format ticks with commas for better readability

# Set plot title and labels
plt.title('Box Plot: Order Total Amount with Outliers')
plt.xlabel('Order Total Amount')
plt.show()

"""### Removing Outliers :"""



Q1 = df['Order Total Amount'].quantile(0.25)
Q3 = df['Order Total Amount'].quantile(0.75)
IQR = Q3 - Q1
df_no_outliers = df[(df['Order Total Amount'] >= Q1 - 1.5 * IQR) & (df['Order Total Amount'] <= Q3 + 1.5 * IQR)]


plt.figure(figsize=(8, 6))
sns.boxplot(x=df_no_outliers['Order Total Amount'],showfliers=False, color='salmon')
plt.title('Box Plot: Order Total Amount after removed Outliers ')
plt.xlabel('Order Total Amount')
plt.show()

"""### Univariate Analysis for Numeric Variables:"""

plt.figure(figsize=(10, 6))
sns.histplot(df['Order Total Amount'], bins=30, kde=True, color='skyblue', stat='count')

plt.gca().xaxis.set_major_formatter(ticker.StrMethodFormatter('{x:,.0f}'))

plt.title('Histogram of Order Total Amount')
plt.xlabel('Order Total Amount')
plt.ylabel('Count')
plt.show()

"""### Univariate Analysis for Categorical Variables:"""

plt.figure(figsize=(8, 6))
sns.countplot(x=df['Order Status'], palette='viridis')
plt.title('Order Status Distribution')
plt.show()

plt.figure(figsize=(12, 6))
sns.countplot(x='Payment Method Title', data=df, palette='magma')
plt.title('Payment Method Distribution')
plt.xlabel('Payment Method')
plt.ylabel('Count')
plt.xticks(rotation=45, ha='right')
plt.show()

top_products = df.groupby('Item Name')['Quantity'].sum().nlargest(10)
plt.figure(figsize=(12, 6))
top_products.plot(kind='bar', color='skyblue')
plt.title('Top Selling Products')
plt.xlabel('Product')
plt.ylabel('Total Quantity Sold')
plt.show()

"""### Bivariate Analysis for Numeric-Numeric Relationships:"""

plt.figure(figsize=(12, 8))
sns.violinplot(x='Order Shipping Amount', y='Quantity', data=df, palette='viridis')
plt.title('Violin Plot: Distribution of Quantity for different Order Shipping Amount levels')
plt.xlabel('Order Shipping Amount')
plt.ylabel('Quantity')
plt.show()

"""### Bivariate Analysis for Categorical-Categorical Relationships:"""

cross_tab = pd.crosstab(df['Order Status'], df['Payment Method Title'])
plt.figure(figsize=(12, 8))
sns.heatmap(cross_tab, annot=True, cmap='YlGnBu', fmt='d', cbar_kws={'label': 'Count'})
plt.title('Cross-Tabulation: Order Status vs Payment Method Title')
plt.xlabel('Payment Method Title')
plt.ylabel('Order Status')
plt.show()

"""### Bivariate Analysis for Numeric-Categorical Relationships:"""

plt.figure(figsize=(12, 8))
sns.barplot(x='Order Status', y='Order Total Amount', data=df, ci=None, palette='pastel')
plt.title('Bar Plot: Average Order Total Amount by Order Status')
plt.xlabel('Order Status')
plt.ylabel('Average Order Total Amount')
plt.show()

"""### Analyzing Sales Over Time:"""

df['Order Date'] = pd.to_datetime(df['Order Date'])
df.set_index('Order Date', inplace=True)

# Resample data to monthly and sum the total sales
monthly_sales = df['Order Total Amount'].resample('M').sum()

plt.figure(figsize=(12, 6))
monthly_sales.plot(kind='line', marker='o', color='green')
plt.title('Monthly Total Sales Over Time')
plt.xlabel('Month')
plt.ylabel('Total Sales')

# Format y-axis labels as integers
plt.gca().yaxis.set_major_formatter(ticker.StrMethodFormatter('{x:,.0f}'))

plt.show()

Cancelled_orders = df[df['Order Status'] == 'Cancelled']
top_Cancelled_products = Cancelled_orders['Item Name'].value_counts().head(10)
plt.figure(figsize=(12, 8))
sns.barplot(x=top_Cancelled_products.values, y=top_Cancelled_products.index, palette='viridis')
plt.title('Top 10 Cancelled Products')
plt.xlabel('Count')
plt.ylabel('Product Name')
plt.show()

company_total_sales = df.groupby('Company').size().reset_index(name='Total Sales')
company_total_sales = company_total_sales.sort_values(by='Total Sales', ascending=False)

plt.figure(figsize=(12, 6))
sns.barplot(x='Total Sales', y='Company', data=company_total_sales)
plt.title('Total Sales by Company')
plt.xlabel('Total Sales')
plt.ylabel('Company')
plt.show()

df.T

df.columns

df['Profit'] = (df['Order Subtotal Amount'])- (df['Item Cost'])

df.T

from sklearn.preprocessing import LabelEncoder
order_status_mapping = {
    'Cancelled': 0,
    'Completed': 1,
    'failed': 2,
    'Cancel request': 0,
    'on hold': 3,
    'processing': 4
}

label_encoder = LabelEncoder()
df['Order Status Numerical'] = label_encoder.fit_transform(df['Order Status'].map(order_status_mapping))
print(df[['Order Status', 'Order Status Numerical']])

df.describe()

df.dtypes

df_encoded = pd.get_dummies(df, columns=['Payment Method Title'])
print(df_encoded)

df_encoded = pd.get_dummies(df, columns=['Shipping Method Title'])
print(df_encoded)

from sklearn.preprocessing import LabelEncoder
order_status_mapping = {
    'Cancelled': 0,
    'Completed': 1,
    'failed': 2,
    'Cancel request': 0,
    'on hold': 3,
    'processing': 4
}

label_encoder = LabelEncoder()
df['Order Status Numerical'] = label_encoder.fit_transform(df['Order Status'].map(order_status_mapping))
print(df[['Order Status', 'Order Status Numerical']])

grouped_data = df.groupby(['City (Billing)', 'City (Shipping)']).agg({'Order Number': 'count'}).reset_index()
grouped_data.rename(columns={'Order Number': 'Order_Count'}, inplace=True)
df = pd.merge(df, grouped_data, on=['City (Billing)', 'City (Shipping)'], how='left')
print(df.head())

df.dtypes

!pip install category_encoders

import pandas as pd
from sklearn.preprocessing import LabelEncoder


def frequency_encode(column):
    frequency_map = column.value_counts(normalize=True)
    return column.map(frequency_map)
categorical_columns = ['Company', 'City (Billing)', 'Category Name']
for col in categorical_columns:
    df[col+'_encoded'] = frequency_encode(df[col])


df.drop(columns=categorical_columns, inplace=True)
df.head()

df.columns

df.head()

df.dtypes

columns_for_correlation = ['Order Number', 'Order Status', 'First Name (Billing)',
       'State Code (Billing)', 'First Name (Shipping)', 'City (Shipping)',
       'Payment Method Title', 'Order Subtotal Amount',
       'Shipping Method Title', 'Order Shipping Amount', 'Order Total Amount',
       'SKU', 'Item Name', 'Quantity', 'Item Cost', 'Profit',
       'Order Status Numerical', 'Company_encoded', 'City (Billing)_encoded',
       'Category Name_encoded']

# Plotting the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Correlation Heatmap')
plt.show()

"""# PCA


1.standarize the data
"""

df.describe()

from sklearn.preprocessing import StandardScaler
import pandas as pd

# Select numerical columns for standardization
numerical_columns = ['Order Subtotal Amount',
                     'Quantity', 'Item Cost',  'Order_Count','Order Status Numerical',
                     'Company_encoded', 'City (Billing)_encoded', 'Category Name_encoded']


scaler = StandardScaler()
scaled_values = scaler.fit_transform(df[numerical_columns])
df_std = pd.DataFrame(scaled_values, columns=numerical_columns)
print(df_std.head())

"""Check if dataset is normolize or not"""

print(df_std[numerical_columns].mean())
print(df_std[numerical_columns].std())

df_std

"""2.Covariance matrix"""

selected_columns = ['Order Subtotal Amount',
                    'Quantity', 'Item Cost',  'Order_Count','Order Status Numerical',
                    'Company_encoded', 'City (Billing)_encoded', 'Category Name_encoded']

# Step 1: Compute the mean of each column
mean_values = df_std[selected_columns].mean()

# Step 2: Subtract the mean from each data point
centered_data = df_std[selected_columns] - mean_values

# Step 3: Compute the covariance matrix
covariance_matrix = np.cov(centered_data, rowvar=False)
print("Covariance matrix:")
print(covariance_matrix)

"""Calculate Eigenvectors and Eigenvalues"""

selected_columns = ['Order Subtotal Amount', 'Quantity', 'Item Cost', 'Order_Count','Order Status Numerical',
                    'Company_encoded', 'City (Billing)_encoded', 'Category Name_encoded']

# Step 1: Standardize the data
mean_values = df_std[selected_columns].mean()
std_dev_values =df_std[selected_columns].std()
standardized_data = (df_std [selected_columns] - mean_values) / std_dev_values

# Step 2: Compute the covariance matrix
covariance_matrix = np.cov(standardized_data, rowvar=False)

# Step 3: Calculate eigenvectors and eigenvalues
eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)

print("Eigenvalues:")
print(eigenvalues)
print("\nEigenvectors:")
print(eigenvectors)

"""Eigenvalues: Each eigenvalue represents the amount of variance captured by its corresponding eigenvector. Larger eigenvalues correspond to principal components that capture more variance in the data.

Eigenvectors: The eigenvectors are provided as a matrix where each column represents an eigenvector. The eigenvectors are sorted based on their corresponding eigenvalues in descending order. Each eigenvector captures a different direction of variance in the data.
"""

import numpy as np
import matplotlib.pyplot as plt

# Example eigenvalues (replace with your eigenvalues)
eigenvalues = np.array([2.11812971,1.73970202, 0.28736783, 0.00304583, 0.75944441, 0.94023544,
 1.01581489, 1.13625986])

# Scree plot
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.plot(np.arange(1, len(eigenvalues) + 1), eigenvalues, marker='o', linestyle='-')
plt.title('Scree Plot')
plt.xlabel('Principal Component Index')
plt.ylabel('Eigenvalue')

# Cumulative explained variance plot
cumulative_variance_ratio = np.cumsum(eigenvalues) / np.sum(eigenvalues)

plt.subplot(1, 2, 2)
plt.plot(np.arange(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio, marker='o', linestyle='-')
plt.title('Cumulative Explained Variance')
plt.xlabel('Number of Principal Components')
plt.ylabel('Cumulative Variance Ratio')
plt.ylim(0, 1)

plt.tight_layout()
plt.show()

# Total variance (sum of all eigenvalues)
total_variance = np.sum(eigenvalues)

# Percentage of variance explained by each principal component
variance_explained = (eigenvalues / total_variance) * 100

# Cumulative percentage of variance explained
cumulative_variance_explained = np.cumsum(variance_explained)

# Print the percentage of variance explained by each principal component
print("Percentage of Variance Explained by Each Principal Component:")
for i, var in enumerate(variance_explained):
    print(f"PC{i+1}: {var:.2f}%")

# Print the cumulative percentage of variance explained
print("\nCumulative Percentage of Variance Explained:")
for i, cum_var in enumerate(cumulative_variance_explained):
    print(f"PC{i+1}: {cum_var:.2f}%")

"""To decide on the number of principal components to retain:
Scree Plot:

The Scree Plot shows the eigenvalues of each principal component (PC).

Look for the "elbow point" on the Scree Plot, which is the point where the eigenvalues start to level off.

The elbow point indicates the optimal number of principal components to retain.

I observed that around 4 principal components

Cumulative Explained Variance:

The Cumulative Explained Variance plot shows the cumulative proportion of variance explained by each principal component.

It helps determine how many principal components are needed to capture a desired amount of variance.

Look for the point where adding additional components results in diminishing returns in terms of explained variance.

first sort eigenvalue and eigen vector by desecending sorting and after then select highest value of eigenvalue and vector.All this is for projecting data into new principal component
"""

#first sort eigenvalue and eigen vector by desecending sorting and after then select highest value of eigenvalue and vector
# Sort eigenvalues and corresponding eigenvectors
sorted_indices = np.argsort(eigenvalues)[::-1]  # Sort eigenvalues in descending order
sorted_eigenvalues = eigenvalues[sorted_indices]
sorted_eigenvectors = eigenvectors[:, sorted_indices]

# Select the top k eigenvectors
k = 7  # Number of principal components you want to retain
selected_eigenvectors = sorted_eigenvectors[:, :k]

# Now selected_eigenvectors contains the top k eigenvectors with the highest eigenvalues

"""Error Resloved:first the dimention of df_std was (1421,21) because it was containing catogorical data.So changed the step-1 and name df_std.this dataframe has standarize data and further use in PCA.Eigenvector dimention was(10,7) that cause error for multiplication.After changing we got df_std dimension match with eigenvector"""

print("Shape of df_std:", df_std.shape)
print("Shape of selected_eigenvectors:", selected_eigenvectors.shape)

# Rename 'Order Subtotal Amount' column to 'Order Amount'
df_std.rename(columns={'Order Subtotal Amount': 'Order Amount'}, inplace=True)

# Compute loadings
loadings = np.dot(df_std, selected_eigenvectors)

# Get absolute loadings for each principal component
absolute_loadings = np.abs(loadings)

# Get the index of features with the highest loadings for each principal component
top_feature_indices = np.argsort(absolute_loadings, axis=1)[:, ::-1]  # Sort indices in descending order

# Get the names of the top features for each principal component
top_feature_names = [[numerical_columns[i] for i in component] for component in top_feature_indices[:, :8]]

print("Top feature names for each principal component:")
for i, features in enumerate(top_feature_names):
    print(f"Principal Component {i+1} Features:", features)

from sklearn.decomposition import PCA

# Fit PCA to your data
pca = PCA(n_components=8)  # Specify the number of principal components
pca.fit(df_std)

# Access the loadings matrix
loadings_matrix = pca.components_

# Inspect the loadings matrix
print("Loadings Matrix:")
print(loadings_matrix)

"""this matrix shows principal component that obtain from performing PCA
Each row represents a principal component, and each column represents the contribution of each original feature to that principal component.

"""

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

loadings_df = pd.DataFrame(loadings_matrix, columns=df_std.columns)


plt.figure(figsize=(12, 8))
sns.heatmap(loadings_df, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)


plt.title('Principal Component Loadings')
plt.xlabel('Features')
plt.ylabel('Principal Components')
plt.show()

"""A positive value in the heatmap indicates a positive correlation between a feature and a principal component.

Features with positive loadings contribute positively to the principal component, meaning they move in the same direction as the principal component.

 loadings helps interpret how strongly each feature contributes to each principal component.

# MODEL
"""

df_std.columns

df_std

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.kernel_ridge import KernelRidge
from sklearn.ensemble import ExtraTreesRegressor, GradientBoostingRegressor, RandomForestRegressor, BaggingRegressor, ExtraTreesRegressor, AdaBoostRegressor
from sklearn.tree import DecisionTreeRegressor
from xgboost import XGBRegressor
from math import sqrt

target_variable = 'Profit'
top_feature_names =['Order Amount', 'Quantity', 'Item Cost','Order Status Numerical', 'Company_encoded', 'City (Billing)_encoded',
       'Category Name_encoded']

y = df[target_variable]
X_top_features = df_std[top_feature_names]
X_train, X_test, y_train, y_test = train_test_split(X_top_features, y, test_size=0.2, random_state=42)
regressors = [
    ('Extra Trees Regressor', ExtraTreesRegressor()),
    ('Gradient Boosting Regressor', GradientBoostingRegressor()),
    ('Random Forest Regressor', RandomForestRegressor()),
    ('XGB Regressor', XGBRegressor()),
    ('Bagging Regressor', BaggingRegressor()),
    ('Decision Tree Regressor', DecisionTreeRegressor())
]


for reg_name, reg in regressors:

    reg.fit(X_train, y_train)


    y_pred = reg.predict(X_test)


    mse = mean_squared_error(y_test, y_pred)
    rmse = sqrt(mse)  # Calculate RMSE
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    # Print the evaluation metrics for each model
    print(f"Evaluation metrics for {reg_name}:")
    print(f"Root Mean Squared Error: {rmse: .4f}")  # Print RMSE
    print(f"Mean Squared Error: {mse: .4f}")
    print(f"Mean Absolute Error: {mae: .4f}")
    print(f"R-squared Score: {r2: .4f}")
    print("--------------------------------------------------------")

X_train #training data which is 80% data will be trained after ..

X_test #model used 20% data

from sklearn.ensemble import ExtraTreesRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np
# Initialize the Extra Trees Regressor
regressor = ExtraTreesRegressor()

# Fit the model on the training data
regressor.fit(X_train, y_train)

# Make predictions on the test data
predictions = regressor.predict(X_test)

# Calculate evaluation metrics
rmse = np.sqrt(mean_squared_error(y_test, predictions))
mae = mean_absolute_error(y_test, predictions)
r2 = r2_score(y_test, predictions)

# Print the evaluation metrics rounded to two decimal places
print("Root Mean Squared Error:", round(rmse, 2))
print("Mean Absolute Error:", round(mae, 2))
print("R-squared Score:", round(r2, 2))

"""#Model Deployment is pending ......"""

# import pickle
# pickle_out = open("Regression.pkl", "wb")
# pickle.dump(regressor, pickle_out)
# pickle_out.close()

!pip install streamlit

import streamlit as st
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

# Define a function to load your data (X_train, y_train, X_test, y_test)
def load_data():
    # Load your data here
    return X_train, y_train, X_test, y_test

# Define a function to train your model
def train_model(X_train, y_train):
    # Initialize the Extra Trees Regressor
    regressor = ExtraTreesRegressor()
    # Fit the model on the training data
    regressor.fit(X_train, y_train)
    return regressor

# Define a function to evaluate the model
def evaluate_model(model, X_test, y_test):
    # Make predictions on the test data
    predictions = model.predict(X_test)
    # Calculate evaluation metrics
    rmse = np.sqrt(mean_squared_error(y_test, predictions))
    mae = mean_absolute_error(y_test, predictions)
    r2 = r2_score(y_test, predictions)
    return rmse, mae, r2

# Load your data
X_train, y_train, X_test, y_test = load_data()

# Train your model
regressor = train_model(X_train, y_train)

# Evaluate your model
rmse, mae, r2 = evaluate_model(regressor, X_test, y_test)

# Display evaluation metrics in Streamlit
st.title('Extra Trees Regressor Model Evaluation')
st.write("Root Mean Squared Error:", round(rmse, 2))
st.write("Mean Absolute Error:", round(mae, 2))
st.write("R-squared Score:", round(r2, 2))

streamlit run EDA_Group_6_Capestone_Sem_4_Final_Model_.ipynb





